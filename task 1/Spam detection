

import numpy as np
import pandas as pd
import chardet

"""reading the data set

"""
file_path = '/content/spam.csv'
with open (file_path, 'rb') as rawdata:
    result = chardet.detect(rawdata.read(500000))
print(result)

sms = pd.read_csv(file_path, encoding = "Windows-1252")

sms.head()

print(sms.v2[71])
print(sms.v1[71])

sms.info()

sms.head()

sms.dropna(how='any', inplace=True, axis=1)

sms.columns=['label', 'message']
sms.head()

sms.info()



sms['label_num'] = sms.label.map({'ham':0, 'spam':1})
print("After Modification:\n")
sms.head()

print("Before Modification:\n")
sms.head()

sms['message_len'] = sms.message.apply(len)
print("After Modification:\n")
sms.head()

"""importing the visualization packages"""

import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style('whitegrid')
plt.style.use('fivethirtyeight')
plt.figure(figsize=(12,8))

sms[sms.label=='ham'].message_len.plot(bins=35, kind='hist', color='blue', label='Ham Messages', alpha=0.5)
sms[sms.label=='spam'].message_len.plot(kind='hist', color='red', label='Spam Messages', alpha=0.5)

plt.legend()
plt.xlabel("Message Length")

sms.describe()

sms[sms.label=='ham'].describe()

sms[sms.label=='spam'].describe()

import string
from nltk.corpus import stopwords


def temp_process(msg):

    STOPWORDS = stopwords.words('english')
    nopunc = [char for char in msg if char not in string.punctuation]
    nopunc = ''.join(nopunc)
    nopunc = ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])
    return nopunc

message = 'This is an example of a message. There are a lot of things here. Words, ALL the Words (AND information). Tons of Information!'

print("MESSAGE:\n", message, "\n\n")

STOPWORDS = ['is', 'an', 'of', 'a', 'the', 'and']

nopunc = [char for char in message if char not in string.punctuation]
print("Remove Punctuation:\n", nopunc, "\n\n")

nopunc = ''.join(nopunc)
print("After Join:\n", nopunc, "\n\n")

nopunc = ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])
print("After Stopword Removal:\n", nopunc, "\n\n")

print("Before Modification:\n")
sms.head()

import nltk
nltk.download('stopwords')

sms['clean_msg'] = sms.message.apply(temp_process)
print("After Modification:\n")
sms.head()

h_words = sms[sms.label=='ham'].clean_msg.apply(lambda x: [word.lower() for word in x.split()])

h_words[0]

from collections import Counter
ham_words = Counter()

for each_word in h_words:
    ham_words.update(each_word)

print(ham_words.most_common(50))

s_words = sms[sms.label=='spam'].clean_msg.apply(lambda x: [word.lower() for word in x.split()])
spam_words = Counter()

for each_word in s_words:
    spam_words.update(each_word)

print(spam_words.most_common(50))

X = sms.clean_msg
y = sms.label_num

print(X.shape)
print(y.shape)

"""applying train split test to the data set """

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,random_state = 42)

print(X_train.shape, X_test.shape)
print(y_train.shape, y_test.shape)

from sklearn.feature_extraction.text import CountVectorizer

vect = CountVectorizer()
vect.fit(X_train)
X_train_dtm = vect.transform(X_train)

X_test_dtm = vect.transform(X_test)

X_test_dtm

X_train_dtm

"""using navie bayes alogrithm """

from sklearn.naive_bayes import MultinomialNB

nb = MultinomialNB()
nb.fit(X_train_dtm, y_train)

y_pred_class = nb.predict(X_test_dtm)

y_pred_class

X_test[2:3]

from sklearn import metrics

metrics.accuracy_score(y_test, y_pred_class)

metrics.confusion_matrix(y_test, y_pred_class)

X_test[y_pred_class > y_test]


#this is the output for the above task 1  which has done in this internship.
